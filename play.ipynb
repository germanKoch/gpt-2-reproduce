{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import pipeline, set_seed, PretrainedConfig\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "os.environ['XDG_CACHE_HOME'] = './cache'\n",
    "os.environ['HF_HOME'] = './cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = './cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache'\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = './cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm language model, and my project will get better with time, but I think there are some things I'd probably do differently. I\"},\n",
       " {'generated_text': \"Hello, I'm language model, but the second we're talking about your own language, will you please define an argument to those, that actually can\"},\n",
       " {'generated_text': \"Hello, I'm language model, and I'm just a programmer. I'm not a machine-in-machine type-converter (although\"},\n",
       " {'generated_text': 'Hello, I\\'m language model, I\\'m a student. You\\'re a language model, and I don\\'t care about your education, your family.\"'},\n",
       " {'generated_text': \"Hello, I'm language model, I'm program.\\n\\nYou can imagine how it's like at this point in your life, your family takes\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', model_kwargs = {\"cache_dir\": './cache'})\n",
    "generator(\n",
    "    \"Hello, I'm language model,\",\n",
    "    max_length=30,\n",
    "    top_k=50,  # Adjusting top-k\n",
    "    do_sample=True,  # Enable sampling,\n",
    "    num_return_sequences=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif hasattr(torch, 'mps') and torch.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        'cpu'\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_gpt2 import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: %s gpt2\n",
      "> Hello, I'm language model, and and the I ( to and and, and such to a \" ( and a.. that to which of\n",
      "> Hello, I'm language model, a a ( which it to has not one I who and to. to I, and one was that he to\n",
      "> Hello, I'm language model, as, for to that it that. and for are we\n",
      ". in, and a you it, we the\n",
      "> Hello, I'm language model, and ( and that for for and and to would the, was the at,.. in in ( the\n",
      "\n",
      "> Hello, I'm language model, of he.,, and and the that the the it, be and to a we by,. that not\n"
     ]
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_sequences, 1)\n",
    "x = tokens.to(\"cpu\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42)\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        topk_probs, topk_indicies = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_indicies, -1, ix)\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "        \n",
    "for i in range(num_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens=tokens)\n",
    "    print(f\"> {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: %s gpt2\n"
     ]
    }
   ],
   "source": [
    "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir='./cache')\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "for hf_key, hf_value in model_hf.state_dict().items():\n",
    "    if any(hf_key.endswith(w) for w in transposed):\n",
    "        hf_value = hf_value.t()\n",
    "    \n",
    "    \n",
    "    result = hf_value.equal(model_dict[hf_key])\n",
    "    assert result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any('transformer.h.0.attn.c_attn.weight'.endswith(w) for w in transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4738,  0.0874,  0.0039,  ..., -0.2592,  0.1517, -0.4100],\n",
       "        [-0.2614,  0.1473,  0.0695,  ..., -0.0164,  0.2170, -0.1924],\n",
       "        [-0.0978,  0.2387,  0.3668,  ...,  0.1991,  0.1043, -0.2400],\n",
       "        ...,\n",
       "        [ 0.0513, -0.0525,  0.1143,  ...,  0.0095,  0.0293, -0.0046],\n",
       "        [-0.0584, -0.0113,  0.0363,  ..., -0.0516, -0.0429,  0.0070],\n",
       "        [ 0.0250, -0.0156, -0.0318,  ...,  0.0319, -0.0475,  0.0198]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hf.state_dict()['transformer.h.0.attn.c_attn.weight'].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4738,  0.0874,  0.0039,  ..., -0.2592,  0.1517, -0.4100],\n",
       "        [-0.2614,  0.1473,  0.0695,  ..., -0.0164,  0.2170, -0.1924],\n",
       "        [-0.0978,  0.2387,  0.3668,  ...,  0.1991,  0.1043, -0.2400],\n",
       "        ...,\n",
       "        [ 0.0513, -0.0525,  0.1143,  ...,  0.0095,  0.0293, -0.0046],\n",
       "        [-0.0584, -0.0113,  0.0363,  ..., -0.0516, -0.0429,  0.0070],\n",
       "        [ 0.0250, -0.0156, -0.0318,  ...,  0.0319, -0.0475,  0.0198]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['transformer.h.0.attn.c_attn.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.h.0.ln_1.weight torch.Size([768])\n",
    "transformer.h.0.ln_1.bias torch.Size([768])\n",
    "transformer.h.0.attn.bias torch.Size([1, 1, 1024, 1024])\n",
    "transformer.h.0.attn.c_attn.weight torch.Size([2304, 768])\n",
    "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.0.ln_2.weight torch.Size([768])\n",
    "transformer.h.0.ln_2.bias torch.Size([768])\n",
    "transformer.h.0.mlp.c_fc.weight torch.Size([3072, 768])\n",
    "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.0.mlp.c_proj.weight torch.Size([768, 3072])\n",
    "transformer.h.0.mlp.c_proj.bias torch.Size([768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
