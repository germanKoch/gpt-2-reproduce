{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import pipeline, set_seed, PretrainedConfig\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from train_gpt2 import *\n",
    "import tiktoken\n",
    "\n",
    "os.environ['XDG_CACHE_HOME'] = './cache'\n",
    "os.environ['HF_HOME'] = './cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = './cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache'\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = './cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/Users/germankochnev/Desktop/projects/chatgpt2/lg/model_19072.pt', map_location='cpu', weights_only=False)\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = checkpoint['model']\n",
    "weights = {k.replace('_orig_mod.', ''): v for k, v in weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_length = 100\n",
    "temperature = 1\n",
    "number_of_sentences = 5\n",
    "\n",
    "\n",
    "prompt = 'Hello! My name is'\n",
    "prompt_encoded = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "prompt_encoded = prompt_encoded.repeat(number_of_sentences, 1)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(generation_length):\n",
    "        out = model(prompt_encoded)\n",
    "        logits = out[0][:, -1, :] / temperature\n",
    "        next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "        prompt_encoded = torch.cat([prompt_encoded, next_token], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! My name is Carolyn and there is no doubt you will notice more posts on near the top. Where I’m from because some posts I have had show through the oil pastels haven’t pressed so hard. We’ve had forums about swimming some ones have side videos of various science experiments on YouTube. Whenever we had side videos of some teachers posting new photos I would link everyone to what we have in the school.\\nWith you hopefully, this also teaches what to look for when you find'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prompt_encoded[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
